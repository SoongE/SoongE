| Paper Title                                                  | Year | Pub   | Who                      |
| ------------------------------------------------------------ | ---- | ----- | ------------------------ |
| Global Context Vision Transformers                           | 2022 | arXiv | [seungmin]()             |
| RegionCLIP: Region-based Language-Image Pretraining([paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Zhong_RegionCLIP_Region-Based_Language-Image_Pretraining_CVPR_2022_paper.pdf)) | 2022 | CVPR  | [seungmin]()             |
| How Well Do Sparse Imagenet Models Transfer?([paper](https://openaccess.thecvf.com/content/CVPR2022/papers/Iofinova_How_Well_Do_Sparse_ImageNet_Models_Transfer_CVPR_2022_paper.pdf)) | 2022 | CVPR  | [seungmin]()             |
| EdgeNeXt: Efficiently Amalgamated CNN-Transformer Architecture for Mobile Vision Applications([paper](https://arxiv.org/pdf/2206.10589.pdf)) | 2022 | arXiv | [seungmin]()             |
| Residual Attention: A Simple but Effective Method for Multi-Label Recognition([paper](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhu_Residual_Attention_A_Simple_but_Effective_Method_for_Multi-Label_Recognition_ICCV_2021_paper.pdf)) | 2021 | ICCV  |                          |
| Patches Are All You Need?([paper](https://arxiv.org/pdf/2201.09792.pdf)) | 2022 | arXiv | [seungmin]()             |
| Recurrence along Depth: Deep Convolutional Neural Networks with Recurrent Layer Aggregation([paper](https://papers.nips.cc/paper/2021/file/582967e09f1b30ca2539968da0a174fa-Paper.pdf)) | 2021 | NIPS  | [seungmin]()             |
| Do Vision Transformers See Like Convolutional Neural Networks?([paper](Do Vision Transformers See Like Convolutional Neural Networks?)) | 2021 | NIPS  | [seungmin]()             |
| Convolutional Normalization: Improving Deep Convolutional Network Robustness and Training([paper](https://papers.nips.cc/paper/2021/file/f23d125da1e29e34c552f448610ff25f-Paper.pdf)) | 2021 | NIPS  | [seungmin]()             |
| On the  Integration of Self-Attention and Convolution        | 2022 | CVPR  | [seungmin](), [yongha]() |
| CMT:  Convolutional Neural Networks Meet Vision Transformers | 2022 | CVPR  | [seungmin]()             |
| Lite  Vision Transformer with Enhanced Self-Attention        | 2022 | CVPR  | [seungmin]()             |
| MViTv2:  Improved Multiscale Vision Transformers for Classification and Detection | 2022 | CVPR  | [seungmin]()             |