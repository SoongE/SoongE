{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Basic Machine Learning with Tensorflow",
      "provenance": [],
      "collapsed_sections": [
        "V0eWTB5iQDNh",
        "yuRjT5FNEHpC",
        "kLs_KBwt3jEU",
        "4kEgDX5GOX8m",
        "syXnpoGP9cOm",
        "A_GjLw_MKtCt",
        "9HHqD_5AVlHk",
        "-3IDmy8OL3mj",
        "fsUHv9UVVrZQ",
        "g2PAs1K8jY-I",
        "zdOl7_Fcte8p",
        "gIU8KIFha7N4",
        "I8ADvKnBZJKA",
        "JY8PZzaihEoC",
        "w0KzmEnBKU-0"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rhcsky/Slef_Study/blob/master/Deep_Learning/NLP/Code/Basic_Machine_Learning_with_Tensorflow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-2La73vYDx8k",
        "colab_type": "text"
      },
      "source": [
        "# For Learning ML with Tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "koCStNwKExVD",
        "colab_type": "code",
        "outputId": "726843c0-7327-40f3-c0df-79baad2bf7f7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%tensorflow_version 1.x\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm as tq\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pprint import pprint as pp"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "TensorFlow 1.x selected.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0eWTB5iQDNh",
        "colab_type": "text"
      },
      "source": [
        "## 1. Linear Regression and Classification\n",
        "Cost function, Gradient descent, Mini batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuRjT5FNEHpC",
        "colab_type": "text"
      },
      "source": [
        "### 1-1. Linear Regression\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XOYoLrWGCY4e",
        "colab_type": "text"
      },
      "source": [
        "Cost function -> MSE <br>\n",
        "How to minimize **cost**?  -> Using Gradient Descent "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6a3qN800EL5a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# x_data = np.array([[1],[2],[3]], dtype=np.float32)\n",
        "# y_data = np.array([[1],[2],[3]], dtype=np.float32)\n",
        "x_data = [1,2,3]\n",
        "y_data = [1,2,3]\n",
        "\n",
        "X = tf.placeholder(tf.float32, name='X_input')\n",
        "Y = tf.placeholder(tf.float32, name='Y_input')\n",
        "\n",
        "W = tf.Variable(tf.random.normal([1]), name='weight')\n",
        "b = tf.Variable(tf.random.normal([1]), name='bias')\n",
        "\n",
        "#Model\n",
        "hypothesis = X*W + b \n",
        "\n",
        "cost = tf.reduce_mean(tf.square(hypothesis-Y)) #Cost function\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
        "\n",
        "## If you want to get gradients\n",
        "# optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.01)\n",
        "# gvs = optimizer.compute_gradients(cost)\n",
        "# apply_gradients = optimizer.apply_gradients(gvs)\n",
        "# sess.run(apply_gradients)\n",
        "\n",
        "with tf.Session() as sess: ## Tensorflow 1에서는 Session을 이용해 계산해 주어야 한다.\n",
        "  sess.run(tf.global_variables_initializer()) ##Initializes global variables in the graph.\n",
        "\n",
        "  for epoch in tq(range(2001)):\n",
        "    sess.run(optimizer, feed_dict={X:x_data,Y:y_data})\n",
        "\n",
        "    if epoch%100 == 0:\n",
        "      print(f'{epoch} W : {sess.run(W)}, b : {sess.run(b)} loss : {sess.run(cost,feed_dict={X:x_data,Y:y_data})}')\n",
        "\n",
        "line_x = np.arange(min(x_data),max(x_data),0.01)\n",
        "line_y = W*line_x + b\n",
        "plt.plot(line_x,line_y,'-r')\n",
        "plt.plot(X,Y,'bo')\n",
        "plt.xlabel('X')\n",
        "plt.ylabel('Y')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oRINi10gnP8P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Tensorflow 2\n",
        "# X = [1,2,3]\n",
        "# Y = [1,2,3]\n",
        "\n",
        "# # Model\n",
        "# W = tf.Variable(tf.random.normal([1]), name='weight')\n",
        "# b = tf.Variable(tf.random.normal([1]), name='bias')\n",
        "\n",
        "# def compute_cost():\n",
        "#   y_pred = W * X + b\n",
        "#   cost = tf.reduce_mean(tf.square(Y-y_pred))\n",
        "#   return cost\n",
        "\n",
        "# optimizer = tf.optimizers.SGD(learning_rate = 0.01)\n",
        "\n",
        "# for i in tq(range(1000)):\n",
        "#   optimizer.minimize(compute_cost, [W,b])\n",
        "\n",
        "#   if i%100 == 0:\n",
        "#     print(f'{i} W : {W.numpy()}, b : {b.numpy()} loss : {compute_cost().numpy()}')\n",
        "\n",
        "# line_x = np.arange(min(X),max(X),0.01)\n",
        "# line_y = W*line_x + b\n",
        "\n",
        "# plt.plot(line_x,line_y,'-r')\n",
        "# plt.plot(X,Y,'bo')\n",
        "# plt.xlabel('X')\n",
        "# plt.ylabel('Y')\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rv74lIf3f56",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kLs_KBwt3jEU",
        "colab_type": "text"
      },
      "source": [
        "###1-2. Multi Varialbe Linear Regression\n",
        "with mini-batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oswv8F4hFYQE",
        "colab_type": "text"
      },
      "source": [
        "Lecture(theory) : H(x) = Wx + b <br>\n",
        "In TensorFlow : H(X) = XW -> Because of matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XcY4tWk_4LKA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data = [[73., 80., 75.],\n",
        "          [93., 88., 93.],\n",
        "          [89., 91., 90.],\n",
        "          [96., 98., 100.],\n",
        "          [73., 66., 70.]]\n",
        "y_data = [[152.],\n",
        "          [185.],\n",
        "          [180.],\n",
        "          [196.],\n",
        "          [142.]]\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=[None,3], name='X_input')\n",
        "Y = tf.placeholder(tf.float32, shape=[None,1], name='Y_input')\n",
        "\n",
        "W = tf.Variable(tf.random.normal([3,1]), name='weight')\n",
        "b = tf.Variable(tf.random.normal([1]), name='bias')\n",
        "\n",
        "hypothesis = tf.matmul(X, W) + b # tf.matmaul => 행렬곱\n",
        "\n",
        "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
        "train_step = tf.train.GradientDescentOptimizer(learning_rate=0.00001).minimize(cost)\n",
        "\n",
        "with tf.Session() as sess:\n",
        " sess.run(tf.global_variables_initializer())\n",
        "\n",
        " for epoch in tq(range(2001)):\n",
        "   cost_val, hy_val, _ = sess.run([cost, hypothesis, train_step], feed_dict={X:x_data,Y:y_data})\n",
        "   \n",
        "   if epoch % 100 == 0:\n",
        "     print(f'{epoch}  | Cost: {cost_val}\\nPrediction:\\n{hy_val}')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BdZtV_c7Lk54",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data load using tf reader\n",
        "\n",
        "filenames = tf.train.string_input_producer(['xxxxx.csv'], \n",
        "                                                shuffle=False, name='filenames')\n",
        "\n",
        "key, value = tf.TextLineReader().read(filenames)\n",
        "\n",
        "record_defaults = [[0.], [0.], [0.], [0.]]\n",
        "data = tf.decode_csv(value, record_defaults=record_defaults)\n",
        "\n",
        "train_x_batch, train_y_batch = tf.train.batch([data[:-1], data[-1:]], batch_size=10)\n",
        "\n",
        "X = tf.placeholder(tf.float32, shape=[None,3], name='X_input')\n",
        "Y = tf.placeholder(tf.float32, shape=[None,1], name='Y_input')\n",
        "\n",
        "W = tf.Variable(tf.random.normal([3,1]), name='weight')\n",
        "b = tf.Variable(tf.random.normal([1]), name='bias')\n",
        "\n",
        "hypothesis = tf.matmul(X, W) + b\n",
        "\n",
        "cost = tf.reduce_mean(tf.square(hypothesis - Y))\n",
        "train_step = tf.train.GradientDescentOptimizer(learning_rate=1e-5).minimize(cost)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "  sess.run(tf.global_variables_initializer())\n",
        "  \n",
        "  # Start population the filenames, Batch를 관리하기 위한 것 같음. 통상적으로 이렇게 사용\n",
        "  coord = tf.train.Coordinator()\n",
        "  threads = tf.train.start_queue_runners(sess=sess, coord = coord)\n",
        "\n",
        "  for epoch in range(2001):\n",
        "      x_batch, y_batch = sess.run([train_x_batch, train_y_batch])\n",
        "      cost_val, hy_val, _ = sess.run(\n",
        "          [cost, hypothesis, train_step], feed_dict={X: x_batch, Y: y_batch})\n",
        "      if epoch % 100 == 0:\n",
        "        print(f'{epoch}  | Cost: {cost_val}\\nPrediction:\\n{hy_val}')\n",
        "\n",
        "  coord.request_stop()\n",
        "  coord.join(threads)\n",
        "\n",
        "\n",
        "## Estimate the value through the model created.\n",
        "print(\"Other scores will be \",\n",
        "      sess.run(hypothesis, feed_dict={X: [[60, 70, 110], [90, 100, 80]]}))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4kEgDX5GOX8m",
        "colab_type": "text"
      },
      "source": [
        "###1-3. Logistic Classification\n",
        "predict 0 or 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgouQmT_Oxl1",
        "colab_type": "text"
      },
      "source": [
        "Using simoid function."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FjnQBDjMObSk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data = [[1,2],[2,3],[3,1],[4,3],[5,3],[6,2]]\n",
        "y_data = [[0],[0],[0],[1],[1],[1]]\n",
        "\n",
        "# placeholders for a tensor that will be always fed.\n",
        "X = tf.placeholder(tf.float32, shape=[None, 2])\n",
        "Y = tf.placeholder(tf.float32, shape=[None, 1])\n",
        "\n",
        "W = tf.Variable(tf.random_normal([2, 1]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([1]), name='bias')\n",
        "\n",
        "# 값을 0~1 사이로 나오게 하기 위해 sigmoid 함수를 취해줌\n",
        "hypothesis = tf.sigmoid(tf.matmul(X, W) + b)\n",
        "\n",
        "# cost/loss function\n",
        "cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
        "train_step = tf.train.GradientDescentOptimizer(learning_rate=0.01).minimize(cost)\n",
        "\n",
        "#cast Y_pred to 0 or 1\n",
        "y_pred = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(y_pred, Y), dtype=tf.float32))\n",
        "\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for eopch in range(10001):\n",
        "        cost_val, _ = sess.run([cost, train_step], feed_dict={X: x_data, Y: y_data})\n",
        "        if epoch % 200 == 0:\n",
        "            print(epoch, cost_val)\n",
        "\n",
        "    h, c, a = sess.run([hypothesis, y_pred, accuracy], feed_dict={X: x_data, Y: y_data})\n",
        "    print(f\"\\nHypothesis: '{h}'\\nCorrect (Y): '{c}'\\nAccuracy: '{a}' \")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syXnpoGP9cOm",
        "colab_type": "text"
      },
      "source": [
        "##2. Softmax Regression\n",
        "predict 0 to 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tsa4mrH8974E",
        "colab_type": "text"
      },
      "source": [
        "softmax has predict value as probability.\n",
        "The sum of all probabilities is 1. <br>\n",
        "Cost function : Cross - Entropy.\n",
        "Optimizer : GD"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "foyH1_MfAiMI",
        "colab_type": "text"
      },
      "source": [
        "Softmax는 이진화된 값(Binary Value)으로 표현하기 위해 One-Hot Encoding이라는 것을 해주어야 한다. <br>\n",
        "단순한 Integer Encoding의 범주형 값(Categorical Value)은 잘못된 경향성을 학습할 수 있다. EX) dog(1), cat(2), horse(3)이 있을 때 dog와 horse의 평균을 cat이라 생각할 수 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GlaQ3Rk6AKS7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load data\n",
        "xy = np.loadtxt('data-04-zoo.csv', delimiter=',', dtype=np.float32)\n",
        "x_data = xy[:, 0:-1]\n",
        "y_data = xy[:, [-1]]\n",
        "\n",
        "nb_classes = 7\n",
        "\n",
        "X = tf.placeholder(tf.float32, [None, 16])\n",
        "Y = tf.placeholder(tf.int32, [None, 1])\n",
        "\n",
        "# In TF, if the input indices is rank N, the output will have rank N+1.\n",
        "# So you have to reshape the output.\n",
        "Y_one_hot = tf.one_hot(Y, nb_classes)\n",
        "Y_one_hot = tf.reshape(Y_one_hot, [-1, nb_classes])\n",
        "\n",
        "W = tf.Variable(tf.random_normal([16, nb_classes]), name='weight')\n",
        "b = tf.Variable(tf.random_normal([nb_classes]), name='bias')\n",
        "\n",
        "logits = tf.matmul(X, W) + b\n",
        "y_pred = tf.nn.softmax(logits)\n",
        "\n",
        "# tf.nn.softmax_cross_entropy_with_logits API -> One-hot Encoding 형태의 정답 레이블일 때 사용.\n",
        "# tf.nn.sparse_softmax_corss_entropy_with_logits API -> One-hot Encoding을 자동으로 수행함으로 labels에 범주형 값 대입.\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits, labels=tf.stop_gradient([Y_one_hot])))\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "# tf.argmax는 probability중 가장 높은 probabliity를 스칼라 형태로 리턴.\n",
        "prediction = tf.argmax(y_pred,1)\n",
        "correct_prediction = tf.equal(tf.argmax(y_pred,1), tf.argmax(Y_one_hot,1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for epoch in range(2001):\n",
        "        _, cost_val, acc_val = sess.run([optimizer, cost, accuracy], feed_dict={X: x_data, Y: y_data})\n",
        "                                        \n",
        "        if epoch % 100 == 0:\n",
        "            print(\"Step: {:5}\\tCost: {:.3f}\\tAcc: {:.2%}\".format(step, cost_val, acc_val))\n",
        "\n",
        "    #올바르게 predict 했는지 확인해보기\n",
        "    pred = sess.run(prediction, feed_dict={X: x_data})\n",
        "    # y_data: (N,1) = flatten => (N, ) matches pred.shape\n",
        "    for p, y in zip(pred, y_data.flatten()):\n",
        "        print(f\"[{p==int(y)}] Prediction: {p} True Y: {int(y)}\")\n",
        "        "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_GjLw_MKtCt",
        "colab_type": "text"
      },
      "source": [
        "##3. Learning rate, Preprocessing(Normalization), Overfitting(Regularization), Dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9HHqD_5AVlHk",
        "colab_type": "text"
      },
      "source": [
        "### 3-1. Theory"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-3IDmy8OL3mj",
        "colab_type": "text"
      },
      "source": [
        "#### Learning rate\n",
        "* Large learning rate - Cost가 계속 증가한다.\n",
        "* Small learning rate - Cost 변화가 너무 없다.\n",
        "` 결국 Learning rate은 Cost를 관찰하여 직접 설정해햐 한다.`\n",
        "-----\n",
        "#### Preprocessing(Normalization of input)\n",
        "* zero-centered data : value의 중심을 0으로 바꿔주는 것\n",
        "```python\n",
        "xy = np.array(...)\n",
        "xy = MinMaxScaler(xy)\n",
        "```\n",
        "* normalized data : value값의 범위를 정해주는 것, 너무 튀는 값은 train data에서 제거해준다.\n",
        "  > Standardization(normalization의 한 종류)\n",
        "  > ```python\n",
        "  > x_std[:,0] = (x[:,0] - x[:,0].mean()) / x[:,0].std()\n",
        "  > ```\n",
        "  > std() = 표준편차\n",
        "------\n",
        "#### Overfitting\n",
        "Regularization은 Overfitting을 줄이기 위해 무거운 weight을 줄여주는 방식. cost함수 뒤에 텀을 추가. Regularization strength를 설정.\n",
        "```python\n",
        "l2reg = Regularization_strength * tf.reduce_sum(tf.square(W))\n",
        "```\n",
        "-----\n",
        "#### Training Data, Validation Data, Test data\n",
        "전체 데이터를 `트레이닝`, `검증`, `테스트`의 총 3가지로 데이터를 분류하는 것. 검증용 데이터는 트레이닝 과정에서 학습에 사용하지는 않지만 중간중간 테스트하는데 사용해서 학습하고있는 모델이 오버피팅에 빠지지 않는지 체크한다.그리고 마지막으로 테스트 데이터를 통해 모델의 정확도를 확인한다.\n",
        "\n",
        "-----\n",
        "\n",
        "#### Epoch, batch size, iterations\n",
        "Epoch = 전체 데이터를 몇번 돌았나<br>\n",
        "batch size = 데이터를 사이즈만큼 나눈다<br>\n",
        "iteration = batch size만큼 잘른 데이터 갯수\n",
        "\n",
        "epoch = num_batch * iteration;\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsUHv9UVVrZQ",
        "colab_type": "text"
      },
      "source": [
        "###3-2. MNIST Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGQ4YTRQVwaw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "nb_classes = 10\n",
        "# MNIST data image of shape (28*28 = 784)\n",
        "X = tf.placeholder(tf.float32,[None,784])\n",
        "Y = tf.placeholder(tf.float32,[None,nb_classes])\n",
        "\n",
        "W = tf.Variable(tf.random_normal([784,nb_classes])) #Input, output 갯수\n",
        "b = tf.Variable(tf.random_normal([nb_classes])) #output 갯수\n",
        "\n",
        "logits = tf.matmul(X, W) + b\n",
        "y_pred = tf.nn.softmax(logits)\n",
        "\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logits)\n",
        "optimizer = tf.train.GradientDescentOptimizer(learning_rate=0.1).minimize(cost)\n",
        "\n",
        "is_correct = tf.equal(tf.argmax(y_pred, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(is_correct, tf.float32))\n",
        "\n",
        "# parameters\n",
        "num_epochs = 15\n",
        "batch_size = 100\n",
        "num_iterations = int(mnist.train.num_examples / batch_size)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "    \n",
        "    for epoch in range(num_epochs):\n",
        "        avg_cost = 0\n",
        "\n",
        "        for i in range(num_iterations):\n",
        "            batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "            _, cost_val = sess.run([optimizer, cost], feed_dict={X: batch_xs, Y: batch_ys})\n",
        "            avg_cost += cost_val / num_iterations\n",
        "\n",
        "        print(f\"Epoch: {epoch+1}, Cost: {avg_cost}\")\n",
        "\n",
        "    print(\n",
        "        \"Accuracy: \",\n",
        "        accuracy.eval( # sess.run(accuracy) 와 같은 기능을 한다.\n",
        "            session=sess, feed_dict={X: mnist.test.images, Y: mnist.test.labels}\n",
        "        ),\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1A-DWhAcLzU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get one and predict\n",
        "r = random.randint(0, mnist.test.num_examples - 1)\n",
        "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r : r + 1], 1)))\n",
        "print(\n",
        "    \"Prediction: \",\n",
        "    sess.run(tf.argmax(y_pred, 1), feed_dict={X: mnist.test.images[r : r + 1]}),\n",
        ")\n",
        "\n",
        "plt.imshow(\n",
        "    mnist.test.images[r : r + 1].reshape(28, 28),\n",
        "    cmap=\"Greys\",\n",
        "    interpolation=\"nearest\",\n",
        ")\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx9wwWffc6cI",
        "colab_type": "text"
      },
      "source": [
        "##4. Deep Learning\n",
        "* Backpropagation\n",
        "* Convolution Neural Networks\n",
        "* Vanishing Gradient\n",
        "> The deeper the layer, the less the input value affects the output value. So we use. So we use Relu instead of Sigmoid as an activation function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twqLeXC4pVmk",
        "colab_type": "text"
      },
      "source": [
        "In NN, we can't use Sigmoid anymore. So there are several activation function to replace it.\n",
        "* tanh\n",
        "* ReLU - max(0,x)\n",
        "* Leaky ReLU - max(0.1x,x)\n",
        "* ELU - f(x) = (a(exp(x)-1),x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2PAs1K8jY-I",
        "colab_type": "text"
      },
      "source": [
        "### 4-1. Tensor Manipulation\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RErF_wIjjc1I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "t = np.array([0.,1.,2.,3.,4.,5.,6.])\n",
        "pp(t)\n",
        "print(t.ndim)# Rank => []의 갯수\n",
        "print(t.shape) # Shape\n",
        "\n",
        "sess = tf.InteractiveSession()\n",
        "with sess.as_default():\n",
        "  t2 = tf.constant([[1,2],[3,4]])\n",
        "  pp(t2)\n",
        "  tf.shape(t2).eval()\n",
        "\n",
        "  x=[[1.,2.],[3.,4.]] # 열 = axis0, 행 axis=1 or -1(가장 안쪽에 있는 축)\n",
        "  tf.reduce_mean(x,axis=0).eval()\n",
        "  tf.reduce_mean(x,axis=-1).eval()\n",
        "\n",
        "  tf.argmax(x,axis=-1) # 가장 큰 값의 위치를 리턴\n",
        "\n",
        "  t = np.array([[[0,1,2],[3,4,5]],[[6,7,8],[9,10,11]]])\n",
        "\n",
        "  tf.reshape(t, shape=[-1,3]).eval()\n",
        "  tf.reshape(t,shape=[-1,1,3]).eval() #보통 가장 안쪽 값을 그대로 가져가면서 shape을 재설정\n",
        "\n",
        "  tf.squeeze([[0],[1],[2]]) # [0,1,2]\n",
        "  tf.expand_dims([0,1,2],1).eval() # [[0],[1],[2]]\n",
        "\n",
        "  tf.one_hot([[0],[1],[2],[0]], depth=3).eval() # [[[1.,0.,0.]],[0.,1.,0.,]]... One_hot은 rank가 하나 증가함\n",
        "  tf.reshape(t, shape=[-1,3]).eval() #So 다시 reshape을 이용해 rank 낮추기\n",
        "\n",
        "  tf.case([1.8,2.2,3.3,4.9], tf.int32).eval() # 1,2,3,4\n",
        "\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGt9eFuJdCbS",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdOl7_Fcte8p",
        "colab_type": "text"
      },
      "source": [
        "### 4.2 XOR using NN with **Tensorboard**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "te6lVi-ItkSK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x_data = np.array([[0, 0], [0, 1], [1, 0], [1, 1]], dtype=np.float32)\n",
        "y_data = np.array([[0], [1], [1], [0]], dtype=np.float32)\n",
        "\n",
        "X = tf.placeholder(tf.float32, [None, 2], name=\"x\")\n",
        "Y = tf.placeholder(tf.float32, [None, 1], name=\"y\")\n",
        "\n",
        "with tf.name_scope(\"Layer1\"):\n",
        "    W1 = tf.Variable(tf.random_normal([2, 2]), name=\"weight1\")\n",
        "    b1 = tf.Variable(tf.random_normal([2]), name=\"bias1\")\n",
        "    layer1 = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
        "\n",
        "# Graph를 그리기 위해 summary에 저장\n",
        "    tf.summary.histogram(\"W1\", W1)\n",
        "    tf.summary.histogram(\"b1\", b1)\n",
        "    tf.summary.histogram(\"Layer1\", layer1)\n",
        "\n",
        "\n",
        "with tf.name_scope(\"Layer2\"):\n",
        "    W2 = tf.Variable(tf.random_normal([2, 1]), name=\"weight2\")\n",
        "    b2 = tf.Variable(tf.random_normal([1]), name=\"bias2\")\n",
        "    hypothesis = tf.sigmoid(tf.matmul(layer1, W2) + b2)\n",
        "\n",
        "    tf.summary.histogram(\"W2\", W2)\n",
        "    tf.summary.histogram(\"b2\", b2)\n",
        "    tf.summary.histogram(\"Hypothesis\", hypothesis)\n",
        "\n",
        "# cost/loss function # 그래프를 그릴 때 scope에 맞게 나누어 준다.\n",
        "with tf.name_scope(\"Cost\"):\n",
        "    cost = -tf.reduce_mean(Y * tf.log(hypothesis) + (1 - Y) * tf.log(1 - hypothesis))\n",
        "    tf.summary.scalar(\"Cost\", cost)\n",
        "\n",
        "with tf.name_scope(\"Train\"):\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate=0.01).minimize(cost)\n",
        "\n",
        "\n",
        "predicted = tf.cast(hypothesis > 0.5, dtype=tf.float32)\n",
        "accuracy = tf.reduce_mean(tf.cast(tf.equal(predicted, Y), dtype=tf.float32))\n",
        "tf.summary.scalar(\"accuracy\", accuracy)\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    summary = tf.summary.merge_all()\n",
        "    writer = tf.summary.FileWriter(\"./logs/xor_logs1\")\n",
        "    writer.add_graph(sess.graph)  # Show the graph\n",
        "\n",
        "    # Initialize TensorFlow variables\n",
        "    sess.run(tf.global_variables_initializer())\n",
        "\n",
        "    for step in range(10001):\n",
        "        _, s, cost_val = sess.run(\n",
        "            [optimizer, summary, cost], feed_dict={X: x_data, Y: y_data}\n",
        "        )\n",
        "        writer.add_summary(s, global_step=step)\n",
        "\n",
        "        if step % 1000 == 0:\n",
        "            print(step, cost_val)\n",
        "\n",
        "    # Accuracy report\n",
        "    h, p, a = sess.run(\n",
        "        [hypothesis, predicted, accuracy], feed_dict={X: x_data, Y: y_data}\n",
        "    )\n",
        "    \n",
        "    print(f\"\\nHypothesis:\\n{h} \\nPredicted:\\n{p} \\nAccuracy:\\n{a}\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qbd4aYQcqnla",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard\n",
        "%tensorboard --logdir='./logs/xor_logs1' --port 8008"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIU8KIFha7N4",
        "colab_type": "text"
      },
      "source": [
        "### 4.3 NN, ReLu, Xavier, Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_9fKjoVbHA_",
        "colab_type": "text"
      },
      "source": [
        "Weight Initialization\n",
        "초기 가중치를 잘못 설정할 경우 Local minimum에 수령할 가능성이 높다\n",
        "> * 초기값을 0으로 -> 여러층으로 나누는 의미 상쇄\n",
        "> * sigmoid activation에서 학습을 반복할 수록 0 or 1로 치우치는 Gradient Vanishing이 발생할 수 있다.\n",
        "> * 표준편차를 줄이면 0.5로 수렴될 가능성이 높다.\n",
        "\n",
        "따라서 다음과 같은 initialization 방법이 있으며 상황에 따라 적절한 것을 채택한다.\n",
        "1. Xavier Initialization - Sigmoid, tanh과 같은 비선형함수에서 효과적인 결과를 보여준다.\n",
        "but, ReLU함수 사용시 0으로 수렴하게 되는 현상 확인\n",
        "> Input과 Output 뉴런 수에 기반하여 초기화 스케일을 정한다. \n",
        "2. He Initialization - ReLU activation function사용시 주로 사용하는 방법. 최근 대부분 모델에서는 He 초기화를 주로 선택한다.\n",
        "> Xavier와 비슷하지만 output size는 고려하지 않는다. \n",
        "3. 최근 Deep CNN모델은 주로 Gaussian Distribution을 따르는 가중치 초기화 방법을 사용한다.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3HxscEjbGmn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "tf.set_random_seed(777)  # reproducibility\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "# parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "total_batch = int(mnist.train.num_examples / batch_size)\n",
        "\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "Y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "keep_prob = tf.placeholder(tf.float32) # test할때는 꼭 1로 해야한다.\n",
        "\n",
        "\n",
        "# 효과적인 weight initializing을 위해 xavier의 initialize 방법을 사용한다.\n",
        "W1 = tf.get_variable(\"W1\", shape=[784, 512],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b1 = tf.Variable(tf.random_normal([512]))\n",
        "L1 = tf.nn.relu(tf.matmul(X, W1) + b1)\n",
        "L1 = tf.nn.dropout(L1, keep_prob=keep_prob)\n",
        "\n",
        "W2 = tf.get_variable(\"W2\", shape=[512, 512],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b2 = tf.Variable(tf.random_normal([512]))\n",
        "L2 = tf.nn.relu(tf.matmul(L1, W2) + b2)\n",
        "L2 = tf.nn.dropout(L2, keep_prob=keep_prob)\n",
        "\n",
        "W3 = tf.get_variable(\"W3\", shape=[512, 512],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b3 = tf.Variable(tf.random_normal([512]))\n",
        "L3 = tf.nn.relu(tf.matmul(L2, W3) + b3)\n",
        "L3 = tf.nn.dropout(L3, keep_prob=keep_prob)\n",
        "\n",
        "W4 = tf.get_variable(\"W4\", shape=[512, 512],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b4 = tf.Variable(tf.random_normal([512]))\n",
        "L4 = tf.nn.relu(tf.matmul(L3, W4) + b4)\n",
        "L4 = tf.nn.dropout(L4, keep_prob=keep_prob)\n",
        "\n",
        "W5 = tf.get_variable(\"W5\", shape=[512, 10],\n",
        "                     initializer=tf.contrib.layers.xavier_initializer())\n",
        "b5 = tf.Variable(tf.random_normal([10]))\n",
        "logits = tf.matmul(L4, W5) + b5\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=logits, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "\n",
        "    for i in range(total_batch):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "        feed_dict = {X: batch_xs, Y: batch_ys, keep_prob: 0.7}\n",
        "        c, _ = sess.run([cost, optimizer], feed_dict=feed_dict)\n",
        "        avg_cost += c / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "\n",
        "# Test model and check accuracy, keep_prob를 1로 유지하는 것 주의!\n",
        "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
        "      X: mnist.test.images, Y: mnist.test.labels, keep_prob: 1}))\n",
        "\n",
        "# Get one and predict\n",
        "r = random.randint(0, mnist.test.num_examples - 1)\n",
        "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
        "print(\"Prediction: \", sess.run(\n",
        "    tf.argmax(logits, 1), feed_dict={X: mnist.test.images[r:r + 1], keep_prob: 1}))\n",
        "\n",
        "# plt.imshow(mnist.test.images[r:r + 1].\n",
        "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I8ADvKnBZJKA",
        "colab_type": "text"
      },
      "source": [
        "### 4.4 Batch Normalization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEJ0e89wZQj9",
        "colab_type": "text"
      },
      "source": [
        "DNN의 문제점 : 초기값을 매우 신중히 선택해야 한다. 이전 네트워크의 파라미터 변화는 다음 네트워크를 지나면서 점점 변화량을 증폭딘다. 따라서 입력 분포의 변화가 일어나면 레이어들은 새로운 분포에 적응해야하는 문제가 있다(Covariate Shift). 즉 정리해보면\n",
        "> 1. 입력 분포의 변화는 일어나지 않는 것이 좋다.\n",
        "> 2. 효율적인 학습을 위해 일정하게 유지는 되는 입력분포 필요\n",
        "> 3. 결국 앞단의 레이어의 출력 분포가 일정해야한다.\n",
        "\n",
        "따라서 Batch normalization을 통해 이러한 \n",
        "문제를 해결할 수 있다.\n",
        "\n",
        "----\n",
        "**Batch Normalization**\n",
        "각각의 Scalar Features를 독립적으로 정규화 하는 방식으로 진행. 각각의 Feature들의 Mean과 Variance를 0과1로 정규화 하는 것.\n",
        "각각의 Activation x에 대해 새로운 파라미터 gamma와 beta를 도입하여 다음과 같은 식으로 연산을 수행한다.\n",
        "\n",
        "$$ y = \\gamma x + \\beta $$\n",
        "\n",
        "Batch Normalization의 유용함은 다음과 같다.\n",
        "* 높은 Learning Rate 설정 가능 : 더 빠른 트레이닝 가능\n",
        "* 트레이닝 시 Deterministic하지 않은 결과 생성 : Regularization 효과로 Dropout을 사용하지 않아도 된다.\n",
        "* Learning Rate Decay를 더 느리게 설정 가능\n",
        "\n",
        "참고 사이트 : https://hcnoh.github.io/2018-11-27-batch-normalization\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S0sRSRwgfriq",
        "colab_type": "text"
      },
      "source": [
        "**1. Define Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GDBv4RIdBzu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Model:\n",
        "    \"\"\"Network Model Class\n",
        "    \n",
        "    Note that this class has only the constructor.\n",
        "    The actual model is defined inside the constructor.\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    X : tf.float32\n",
        "        This is a tensorflow placeholder for MNIST images\n",
        "        Expected shape is [None, 784]\n",
        "        \n",
        "    y : tf.float32\n",
        "        This is a tensorflow placeholder for MNIST labels (one hot encoded)\n",
        "        Expected shape is [None, 10]\n",
        "        \n",
        "    mode : tf.bool\n",
        "        This is used for the batch normalization\n",
        "        It's `True` at training time and `False` at test time\n",
        "        \n",
        "    loss : tf.float32\n",
        "        The loss function is a softmax cross entropy\n",
        "        \n",
        "    train_op\n",
        "        This is simply the training op that minimizes the loss\n",
        "        \n",
        "    accuracy : tf.float32\n",
        "        The accuracy operation\n",
        "        \n",
        "    \n",
        "    Examples\n",
        "    ----------\n",
        "    >>> model = Model(\"Batch Norm\", 32, 10)\n",
        "\n",
        "    \"\"\"\n",
        "    def __init__(self, name, input_dim, output_dim, hidden_dims=[32, 32], use_batchnorm=True, activation_fn=tf.nn.relu, optimizer=tf.train.AdamOptimizer, lr=0.01):\n",
        "        \"\"\" Constructor\n",
        "        \n",
        "        Parameters\n",
        "        --------\n",
        "        name : str\n",
        "            The name of this network\n",
        "            The entire network will be created under `tf.variable_scope(name)`\n",
        "            \n",
        "        input_dim : int\n",
        "            The input dimension\n",
        "            In this example, 784\n",
        "        \n",
        "        output_dim : int\n",
        "            The number of output labels\n",
        "            There are 10 labels\n",
        "            \n",
        "        hidden_dims : list (default: [32, 32])\n",
        "            len(hidden_dims) = number of layers\n",
        "            each element is the number of hidden units\n",
        "            \n",
        "        use_batchnorm : bool (default: True)\n",
        "            If true, it will create the batchnormalization layer\n",
        "            \n",
        "        activation_fn : TF functions (default: tf.nn.relu)\n",
        "            Activation Function\n",
        "            \n",
        "        optimizer : TF optimizer (default: tf.train.AdamOptimizer)\n",
        "            Optimizer Function\n",
        "            \n",
        "        lr : float (default: 0.01)\n",
        "            Learning rate\n",
        "        \n",
        "        \"\"\"\n",
        "        with tf.variable_scope(name):\n",
        "            # Placeholders are defined\n",
        "            self.X = tf.placeholder(tf.float32, [None, input_dim], name='X')\n",
        "            self.y = tf.placeholder(tf.float32, [None, output_dim], name='y')\n",
        "            self.mode = tf.placeholder(tf.bool, name='train_mode')            \n",
        "            \n",
        "            # Loop over hidden layers\n",
        "            net = self.X\n",
        "            for i, h_dim in enumerate(hidden_dims):\n",
        "                with tf.variable_scope('layer{}'.format(i)):\n",
        "                    net = tf.layers.dense(net, h_dim)\n",
        "                    \n",
        "                    if use_batchnorm:\n",
        "                        net = tf.layers.batch_normalization(net, training=self.mode)\n",
        "                        \n",
        "                    net = activation_fn(net)\n",
        "            \n",
        "            # Attach fully connected layers\n",
        "            net = tf.contrib.layers.flatten(net)\n",
        "            net = tf.layers.dense(net, output_dim)\n",
        "            \n",
        "            self.loss = tf.nn.softmax_cross_entropy_with_logits(logits=net, labels=self.y)\n",
        "            self.loss = tf.reduce_mean(self.loss, name='loss')    \n",
        "            \n",
        "            # When using the batchnormalization layers,\n",
        "            # it is necessary to manually add the update operations\n",
        "            # because the moving averages are not included in the graph            \n",
        "            update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope=name)\n",
        "            with tf.control_dependencies(update_ops):                     \n",
        "                self.train_op = optimizer(lr).minimize(self.loss)\n",
        "            \n",
        "            # Accuracy etc \n",
        "            softmax = tf.nn.softmax(net, name='softmax')\n",
        "            self.accuracy = tf.equal(tf.argmax(softmax, 1), tf.argmax(self.y, 1))\n",
        "            self.accuracy = tf.reduce_mean(tf.cast(self.accuracy, tf.float32))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DFLktA8Bfw6j",
        "colab_type": "text"
      },
      "source": [
        "**2. Define Solver**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zIkwzOM8eiU_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Solver:\n",
        "    \"\"\"Solver class\n",
        "    \n",
        "    This class will contain the model class and session\n",
        "    \n",
        "    Attributes\n",
        "    ----------\n",
        "    model : Model class\n",
        "    sess : TF session\n",
        "        \n",
        "    Methods\n",
        "    ----------\n",
        "    train(X, y)\n",
        "        Run the train_op and Returns the loss\n",
        "        \n",
        "    evaluate(X, y, batch_size=None)\n",
        "        Returns \"Loss\" and \"Accuracy\"\n",
        "        If batch_size is given, it's computed using batch_size\n",
        "        because most GPU memories cannot handle the entire training data at once\n",
        "            \n",
        "    Example\n",
        "    ----------\n",
        "    >>> sess = tf.InteractiveSession()\n",
        "    >>> model = Model(\"BatchNorm\", 32, 10)\n",
        "    >>> solver = Solver(sess, model)\n",
        "    \n",
        "    # Train\n",
        "    >>> solver.train(X, y)\n",
        "    \n",
        "    # Evaluate\n",
        "    >>> solver.evaluate(X, y)\n",
        "    \"\"\"\n",
        "    def __init__(self, sess, model):\n",
        "        self.model = model\n",
        "        self.sess = sess\n",
        "        \n",
        "    def train(self, X, y):\n",
        "        feed = {\n",
        "            self.model.X: X,\n",
        "            self.model.y: y,\n",
        "            self.model.mode: True\n",
        "        }\n",
        "        train_op = self.model.train_op\n",
        "        loss = self.model.loss\n",
        "        \n",
        "        return self.sess.run([train_op, loss], feed_dict=feed)\n",
        "    \n",
        "    def evaluate(self, X, y, batch_size=None):\n",
        "        if batch_size:\n",
        "            N = X.shape[0]\n",
        "            \n",
        "            total_loss = 0\n",
        "            total_acc = 0\n",
        "            \n",
        "            for i in range(0, N, batch_size):\n",
        "                X_batch = X[i:i + batch_size]\n",
        "                y_batch = y[i:i + batch_size]\n",
        "                \n",
        "                feed = {\n",
        "                    self.model.X: X_batch,\n",
        "                    self.model.y: y_batch,\n",
        "                    self.model.mode: False\n",
        "                }\n",
        "                \n",
        "                loss = self.model.loss\n",
        "                accuracy = self.model.accuracy\n",
        "                \n",
        "                step_loss, step_acc = self.sess.run([loss, accuracy], feed_dict=feed)\n",
        "                \n",
        "                total_loss += step_loss * X_batch.shape[0]\n",
        "                total_acc += step_acc * X_batch.shape[0]\n",
        "            \n",
        "            total_loss /= N\n",
        "            total_acc /= N\n",
        "            \n",
        "            return total_loss, total_acc\n",
        "            \n",
        "            \n",
        "        else:\n",
        "            feed = {\n",
        "                self.model.X: X,\n",
        "                self.model.y: y,\n",
        "                self.model.mode: False\n",
        "            }\n",
        "            \n",
        "            loss = self.model.loss            \n",
        "            accuracy = self.model.accuracy\n",
        "\n",
        "            return self.sess.run([loss, accuracy], feed_dict=feed)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvigEBLrfzna",
        "colab_type": "text"
      },
      "source": [
        "**3. Instantiate Model/Solver classes**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUdTkwnxf5Q3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "input_dim = 784\n",
        "output_dim = 10\n",
        "N = 55000\n",
        "\n",
        "tf.reset_default_graph()\n",
        "sess = tf.InteractiveSession()\n",
        "\n",
        "# We create two models: one with the batch norm and other without\n",
        "bn = Model('batchnorm', input_dim, output_dim, use_batchnorm=True)\n",
        "nn = Model('no_norm', input_dim, output_dim, use_batchnorm=False)\n",
        "\n",
        "# We create two solvers: to train both models at the same time for comparison\n",
        "# Usually we only need one solver class\n",
        "bn_solver = Solver(sess, bn)\n",
        "nn_solver = Solver(sess, nn)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CDVSIYNBf5OL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "epoch_n = 10\n",
        "batch_size = 32\n",
        "\n",
        "# Save Losses and Accuracies every epoch\n",
        "# We are going to plot them later\n",
        "train_losses = []\n",
        "train_accs = []\n",
        "\n",
        "valid_losses = []\n",
        "valid_accs = []"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c-u9d2J-gHH9",
        "colab_type": "text"
      },
      "source": [
        "**4. Run training**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7dQgakOgOG-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "init = tf.global_variables_initializer()\n",
        "sess.run(init)\n",
        "\n",
        "for epoch in range(epoch_n):\n",
        "    for _ in range(N//batch_size):\n",
        "        X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
        "        \n",
        "        _, bn_loss = bn_solver.train(X_batch, y_batch)\n",
        "        _, nn_loss = nn_solver.train(X_batch, y_batch)       \n",
        "    \n",
        "    b_loss, b_acc = bn_solver.evaluate(mnist.train.images, mnist.train.labels, batch_size)\n",
        "    n_loss, n_acc = nn_solver.evaluate(mnist.train.images, mnist.train.labels, batch_size)\n",
        "    \n",
        "    # Save train losses/acc\n",
        "    train_losses.append([b_loss, n_loss])\n",
        "    train_accs.append([b_acc, n_acc])\n",
        "    print(f'[Epoch {epoch}-TRAIN] Batchnorm Loss(Acc): {b_loss:.5f}({b_acc:.2%}) vs No Batchnorm Loss(Acc): {n_loss:.5f}({n_acc:.2%})')\n",
        "    \n",
        "    b_loss, b_acc = bn_solver.evaluate(mnist.validation.images, mnist.validation.labels)\n",
        "    n_loss, n_acc = nn_solver.evaluate(mnist.validation.images, mnist.validation.labels)\n",
        "    \n",
        "    # Save valid losses/acc\n",
        "    valid_losses.append([b_loss, n_loss])\n",
        "    valid_accs.append([b_acc, n_acc])\n",
        "    print(f'[Epoch {epoch}-VALID] Batchnorm Loss(Acc): {b_loss:.5f}({b_acc:.2%}) vs No Batchnorm Loss(Acc): {n_loss:.5f}({n_acc:.2%})')\n",
        "    print()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JY8PZzaihEoC",
        "colab_type": "text"
      },
      "source": [
        "### 4.5 high level api"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BXIOuOEBtpjK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Lab 10 MNIST and High-level TF API\n",
        "from tensorflow.contrib.layers import fully_connected, batch_norm, dropout\n",
        "from tensorflow.contrib.framework import arg_scope\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "tf.set_random_seed(777)  # reproducibility\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "# parameters\n",
        "learning_rate = 0.01  # we can use large learning rate using Batch Normalization\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "keep_prob = 0.7\n",
        "\n",
        "# input place holders\n",
        "X = tf.placeholder(tf.float32, [None, 784])\n",
        "Y = tf.placeholder(tf.float32, [None, 10])\n",
        "train_mode = tf.placeholder(tf.bool, name='train_mode')\n",
        "\n",
        "# layer output size\n",
        "hidden_output_size = 512\n",
        "final_output_size = 10\n",
        "\n",
        "xavier_init = tf.contrib.layers.xavier_initializer()\n",
        "bn_params = {\n",
        "    'is_training': train_mode,\n",
        "    'decay': 0.9,\n",
        "    'updates_collections': None\n",
        "}\n",
        "\n",
        "# We can build short code using 'arg_scope' to avoid duplicate code\n",
        "# same function with different arguments\n",
        "with arg_scope([fully_connected],\n",
        "               activation_fn=tf.nn.relu,\n",
        "               weights_initializer=xavier_init,\n",
        "               biases_initializer=None,\n",
        "               normalizer_fn=batch_norm,\n",
        "               normalizer_params=bn_params\n",
        "               ):\n",
        "    hidden_layer1 = fully_connected(X, hidden_output_size, scope=\"h1\")\n",
        "    h1_drop = dropout(hidden_layer1, keep_prob, is_training=train_mode)\n",
        "    hidden_layer2 = fully_connected(h1_drop, hidden_output_size, scope=\"h2\")\n",
        "    h2_drop = dropout(hidden_layer2, keep_prob, is_training=train_mode)\n",
        "    hidden_layer3 = fully_connected(h2_drop, hidden_output_size, scope=\"h3\")\n",
        "    h3_drop = dropout(hidden_layer3, keep_prob, is_training=train_mode)\n",
        "    hidden_layer4 = fully_connected(h3_drop, hidden_output_size, scope=\"h4\")\n",
        "    h4_drop = dropout(hidden_layer4, keep_prob, is_training=train_mode)\n",
        "    hypothesis = fully_connected(h4_drop, final_output_size, \n",
        "                                 activation_fn=None, scope=\"hypothesis\")\n",
        "\n",
        "\n",
        "# define cost/loss & optimizer\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "    logits=hypothesis, labels=Y))\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
        "\n",
        "# initialize\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "# train my model\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(mnist.train.num_examples / batch_size)\n",
        "\n",
        "    ##!!!!! Optimizing과 Cost를 계산할 때 따로 running해야함을 주의하라!!!!!\n",
        "    for i in range(total_batch):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "        feed_dict_train = {X: batch_xs, Y: batch_ys, train_mode: True}\n",
        "        feed_dict_cost = {X: batch_xs, Y: batch_ys, train_mode: False}\n",
        "        opt = sess.run(optimizer, feed_dict=feed_dict_train)\n",
        "        c = sess.run(cost, feed_dict=feed_dict_cost)\n",
        "        avg_cost += c / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "\n",
        "\n",
        "# Test model and check accuracy\n",
        "correct_prediction = tf.equal(tf.argmax(hypothesis, 1), tf.argmax(Y, 1))\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "print('Accuracy:', sess.run(accuracy, feed_dict={\n",
        "      X: mnist.test.images, Y: mnist.test.labels, train_mode: False}))\n",
        "\n",
        "# Get one and predict\n",
        "r = random.randint(0, mnist.test.num_examples - 1)\n",
        "print(\"Label: \", sess.run(tf.argmax(mnist.test.labels[r:r + 1], 1)))\n",
        "print(\"Prediction: \", sess.run(\n",
        "    tf.argmax(hypothesis, 1), feed_dict={X: mnist.test.images[r:r + 1], train_mode: False}))\n",
        "\n",
        "# plt.imshow(mnist.test.images[r:r + 1].\n",
        "#           reshape(28, 28), cmap='Greys', interpolation='nearest')\n",
        "# plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6SyxQhclEEL6",
        "colab_type": "text"
      },
      "source": [
        "## 5. CNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w0KzmEnBKU-0",
        "colab_type": "text"
      },
      "source": [
        "### 5-1. Deep CNN\n",
        "3 Convolutional Layer<br>\n",
        "2 Fully connected layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qfe9cGS7JjUs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "tf.set_random_seed(777) \n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "# hyper parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 15\n",
        "batch_size = 100\n",
        "\n",
        "class Model:\n",
        "\n",
        "    def __init__(self, sess, name):\n",
        "        self.sess = sess\n",
        "        self.name = name\n",
        "        self._build_net()\n",
        "\n",
        "    def _build_net(self):\n",
        "        with tf.variable_scope(self.name):\n",
        "\n",
        "            self.keep_prob = tf.placeholder(tf.float32)\n",
        "\n",
        "            # input place holders\n",
        "            self.X = tf.placeholder(tf.float32, [None, 784])\n",
        "            # img 28x28x1 (black/white)\n",
        "            X_img = tf.reshape(self.X, [-1, 28, 28, 1])\n",
        "            self.Y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "            # L1 ImgIn shape=(?, 28, 28, 1)\n",
        "            W1 = tf.Variable(tf.random_normal([3, 3, 1, 32], stddev=0.01))\n",
        "            #    Conv     -> (?, 28, 28, 32) 32는 필터 개수\n",
        "            #    Pool     -> (?, 14, 14, 32)\n",
        "            L1 = tf.nn.conv2d(X_img, W1, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            L1 = tf.nn.relu(L1)\n",
        "            L1 = tf.nn.max_pool(L1, ksize=[1, 2, 2, 1],\n",
        "                                strides=[1, 2, 2, 1], padding='SAME')\n",
        "            L1 = tf.nn.dropout(L1, keep_prob=self.keep_prob)\n",
        "\n",
        "            # L2 ImgIn shape=(?, 14, 14, 32)\n",
        "            W2 = tf.Variable(tf.random_normal([3, 3, 32, 64], stddev=0.01))\n",
        "            #    Conv      ->(?, 14, 14, 64)\n",
        "            #    Pool      ->(?, 7, 7, 64)\n",
        "            L2 = tf.nn.conv2d(L1, W2, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            L2 = tf.nn.relu(L2)\n",
        "            L2 = tf.nn.max_pool(L2, ksize=[1, 2, 2, 1],\n",
        "                                strides=[1, 2, 2, 1], padding='SAME')\n",
        "            L2 = tf.nn.dropout(L2, keep_prob=self.keep_prob)\n",
        "\n",
        "            # L3 ImgIn shape=(?, 7, 7, 64)\n",
        "            W3 = tf.Variable(tf.random_normal([3, 3, 64, 128], stddev=0.01))\n",
        "            #    Conv      ->(?, 7, 7, 128)\n",
        "            #    Pool      ->(?, 4, 4, 128)\n",
        "            #    Reshape   ->(?, 4 * 4 * 128) # Flatten them for FC\n",
        "            L3 = tf.nn.conv2d(L2, W3, strides=[1, 1, 1, 1], padding='SAME')\n",
        "            L3 = tf.nn.relu(L3)\n",
        "            L3 = tf.nn.max_pool(L3, ksize=[1, 2, 2, 1], strides=[\n",
        "                                1, 2, 2, 1], padding='SAME')\n",
        "            L3 = tf.nn.dropout(L3, keep_prob=self.keep_prob)\n",
        "\n",
        "            L3_flat = tf.reshape(L3, [-1, 4 * 4 * 128]) # 쭉 펼쳐준다.\n",
        "\n",
        "            # L4 FC 4x4x128 inputs -> 625 outputs\n",
        "            W4 = tf.get_variable(\"W4\", shape=[128 * 4 * 4, 625],\n",
        "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
        "            b4 = tf.Variable(tf.random_normal([625]))\n",
        "            L4 = tf.nn.relu(tf.matmul(L3_flat, W4) + b4)\n",
        "            L4 = tf.nn.dropout(L4, keep_prob=self.keep_prob)\n",
        "\n",
        "            # L5 Final FC 625 inputs -> 10 outputs\n",
        "            W5 = tf.get_variable(\"W5\", shape=[625, 10],\n",
        "                                 initializer=tf.contrib.layers.xavier_initializer())\n",
        "            b5 = tf.Variable(tf.random_normal([10]))\n",
        "            self.logits = tf.matmul(L4, W5) + b5\n",
        "\n",
        "        # define cost/loss & optimizer\n",
        "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "            logits=self.logits, labels=self.Y))\n",
        "        self.optimizer = tf.train.AdamOptimizer(\n",
        "            learning_rate=learning_rate).minimize(self.cost)\n",
        "\n",
        "        correct_prediction = tf.equal(\n",
        "            tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "    def predict(self, x_test, keep_prop=1.0):\n",
        "        return self.sess.run(self.logits, feed_dict={self.X: x_test, self.keep_prob: keep_prop})\n",
        "\n",
        "    def get_accuracy(self, x_test, y_test, keep_prop=1.0):\n",
        "        return self.sess.run(self.accuracy, feed_dict={self.X: x_test, self.Y: y_test, self.keep_prob: keep_prop})\n",
        "\n",
        "    def train(self, x_data, y_data, keep_prop=0.7):\n",
        "        return self.sess.run([self.cost, self.optimizer], feed_dict={\n",
        "            self.X: x_data, self.Y: y_data, self.keep_prob: keep_prop})\n",
        "\n",
        "# initialize\n",
        "sess = tf.Session()\n",
        "m1 = Model(sess, \"m1\")\n",
        "\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "print('Learning Started!')\n",
        "\n",
        "# train my model\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost = 0\n",
        "    total_batch = int(mnist.train.num_examples / batch_size)\n",
        "\n",
        "    for i in range(total_batch):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "        c, _ = m1.train(batch_xs, batch_ys)\n",
        "        avg_cost += c / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', '{:.9f}'.format(avg_cost))\n",
        "\n",
        "print('Learning Finished!')\n",
        "\n",
        "# Test model and check accuracy\n",
        "print('Accuracy:', m1.get_accuracy(mnist.test.images, mnist.test.labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6OLjEo9lGceI",
        "colab_type": "text"
      },
      "source": [
        "### 5.2 Class, Layers, Ensemble"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gAysUgk7HqC9",
        "colab_type": "text"
      },
      "source": [
        "Ensemble : 여러가지 학습한 모델을 통해 예측한 후, 예측값들을 적절히 조화롭게 합치는것을 의미. 가장 기본적인 방법은 Softmax에서 모두의 값을 합하는 것.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EvAnMOBpIxOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.examples.tutorials.mnist import input_data\n",
        "\n",
        "tf.set_random_seed(777)\n",
        "\n",
        "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
        "\n",
        "# hyper parameters\n",
        "learning_rate = 0.001\n",
        "training_epochs = 20\n",
        "batch_size = 100\n",
        "\n",
        "class Model:\n",
        "\n",
        "    def __init__(self, sess, name):\n",
        "        self.sess = sess\n",
        "        self.name = name\n",
        "        self._build_net()\n",
        "\n",
        "    def _build_net(self):\n",
        "        with tf.variable_scope(self.name):\n",
        "\n",
        "            self.training = tf.placeholder(tf.bool)\n",
        "\n",
        "            self.X = tf.placeholder(tf.float32, [None, 784])\n",
        "\n",
        "            # img 28x28x1 (black/white), Input Layer\n",
        "            X_img = tf.reshape(self.X, [-1, 28, 28, 1])\n",
        "            self.Y = tf.placeholder(tf.float32, [None, 10])\n",
        "\n",
        "            # Convolutional Layer #1\n",
        "            conv1 = tf.layers.conv2d(inputs=X_img, filters=32, kernel_size=[3, 3],\n",
        "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
        "            # Pooling Layer #1\n",
        "            pool1 = tf.layers.max_pooling2d(inputs=conv1, pool_size=[2, 2],\n",
        "                                            padding=\"SAME\", strides=2)\n",
        "            dropout1 = tf.layers.dropout(inputs=pool1,\n",
        "                                         rate=0.3, training=self.training)\n",
        "\n",
        "            # Convolutional Layer #2 and Pooling Layer #2\n",
        "            conv2 = tf.layers.conv2d(inputs=dropout1, filters=64, kernel_size=[3, 3],\n",
        "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
        "            pool2 = tf.layers.max_pooling2d(inputs=conv2, pool_size=[2, 2],\n",
        "                                            padding=\"SAME\", strides=2)\n",
        "            dropout2 = tf.layers.dropout(inputs=pool2,\n",
        "                                         rate=0.3, training=self.training)\n",
        "\n",
        "            # Convolutional Layer #3 and Pooling Layer #3\n",
        "            conv3 = tf.layers.conv2d(inputs=dropout2, filters=128, kernel_size=[3, 3],\n",
        "                                     padding=\"SAME\", activation=tf.nn.relu)\n",
        "            pool3 = tf.layers.max_pooling2d(inputs=conv3, pool_size=[2, 2],\n",
        "                                            padding=\"SAME\", strides=2)\n",
        "            dropout3 = tf.layers.dropout(inputs=pool3,\n",
        "                                         rate=0.3, training=self.training)\n",
        "\n",
        "            # Dense Layer with Relu\n",
        "            flat = tf.reshape(dropout3, [-1, 128 * 4 * 4])\n",
        "            dense4 = tf.layers.dense(inputs=flat,\n",
        "                                     units=625, activation=tf.nn.relu)\n",
        "            dropout4 = tf.layers.dropout(inputs=dense4,\n",
        "                                         rate=0.5, training=self.training)\n",
        "\n",
        "            # Logits (no activation) Layer: L5 Final FC 625 inputs -> 10 outputs\n",
        "            self.logits = tf.layers.dense(inputs=dropout4, units=10)\n",
        "\n",
        "        # define cost/loss & optimizer\n",
        "        self.cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
        "            logits=self.logits, labels=self.Y))\n",
        "        self.optimizer = tf.train.AdamOptimizer(\n",
        "            learning_rate=learning_rate).minimize(self.cost)\n",
        "\n",
        "        correct_prediction = tf.equal(\n",
        "            tf.argmax(self.logits, 1), tf.argmax(self.Y, 1))\n",
        "        self.accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
        "\n",
        "    def predict(self, x_test, training=False):\n",
        "        return self.sess.run(self.logits,\n",
        "                             feed_dict={self.X: x_test, self.training: training})\n",
        "\n",
        "    def get_accuracy(self, x_test, y_test, training=False):\n",
        "        return self.sess.run(self.accuracy,\n",
        "                             feed_dict={self.X: x_test,\n",
        "                                        self.Y: y_test, self.training: training})\n",
        "\n",
        "    def train(self, x_data, y_data, training=True):\n",
        "        return self.sess.run([self.cost, self.optimizer], feed_dict={\n",
        "            self.X: x_data, self.Y: y_data, self.training: training})\n",
        "\n",
        "# initialize\n",
        "sess = tf.Session()\n",
        "\n",
        "models = []\n",
        "num_models = 2\n",
        "for m in range(num_models):\n",
        "    models.append(Model(sess, \"model\" + str(m)))\n",
        "\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "print('Learning Started!')\n",
        "\n",
        "# train my model\n",
        "for epoch in range(training_epochs):\n",
        "    avg_cost_list = np.zeros(len(models))\n",
        "    total_batch = int(mnist.train.num_examples / batch_size)\n",
        "    for i in range(total_batch):\n",
        "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
        "\n",
        "        # train each model\n",
        "        for m_idx, m in enumerate(models):\n",
        "            c, _ = m.train(batch_xs, batch_ys)\n",
        "            avg_cost_list[m_idx] += c / total_batch\n",
        "\n",
        "    print('Epoch:', '%04d' % (epoch + 1), 'cost =', avg_cost_list)\n",
        "\n",
        "print('Learning Finished!')\n",
        "\n",
        "# Test model and check accuracy\n",
        "test_size = len(mnist.test.labels)\n",
        "predictions = np.zeros([test_size, 10])\n",
        "for m_idx, m in enumerate(models):\n",
        "    print(m_idx, 'Accuracy:', m.get_accuracy(\n",
        "        mnist.test.images, mnist.test.labels))\n",
        "    p = m.predict(mnist.test.images)\n",
        "    predictions += p\n",
        "\n",
        "ensemble_correct_prediction = tf.equal(\n",
        "    tf.argmax(predictions, 1), tf.argmax(mnist.test.labels, 1))\n",
        "ensemble_accuracy = tf.reduce_mean(\n",
        "    tf.cast(ensemble_correct_prediction, tf.float32))\n",
        "print('Ensemble accuracy:', sess.run(ensemble_accuracy))\n",
        "\n",
        "'''\n",
        "0 Accuracy: 0.9933\n",
        "1 Accuracy: 0.9946\n",
        "2 Accuracy: 0.9934\n",
        "3 Accuracy: 0.9935\n",
        "4 Accuracy: 0.9935\n",
        "5 Accuracy: 0.9949\n",
        "6 Accuracy: 0.9941\n",
        "Ensemble accuracy: 0.9952\n",
        "'''"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lwo2_ot8KfB6",
        "colab_type": "text"
      },
      "source": [
        "##6. RNN\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UIqhWzrsM8HN",
        "colab_type": "text"
      },
      "source": [
        "활용 분야\n",
        "* Language Modeling\n",
        "* Speech Recognition\n",
        "* Machine Translation\n",
        "* Conversation Modeling / Question Answering\n",
        "* Image/Video Captioning\n",
        "* Image/Music/Dence Generation\n",
        "\n",
        "활용 방법\n",
        "* Ont to One - Vanila NN\n",
        "* One to Many - Image Captioning\n",
        "* Many to One - Sentiment Classification\n",
        "* Many to Many - Machine Translation\n",
        "* Many to Many - Video Classification on frame level\n",
        "\n",
        "다양한 모델들\n",
        "* Vanila\n",
        "* LSTM(Long Short Term Memory)\n",
        "* GRU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "twEJffVjKg6a",
        "colab_type": "text"
      },
      "source": [
        "### 6-1. RNN Basic\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1_Ih9_iNypi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from tensorflow.contrib import rnn\n",
        "\n",
        "tf.set_random_seed(777)  # reproducibility\n",
        "\n",
        "sentence = (\"if you want to build a ship, don't drum up people together to \"\n",
        "            \"collect wood and don't assign them tasks and work, but rather \"\n",
        "            \"teach them to long for the endless immensity of the sea.\")\n",
        "\n",
        "char_set = list(set(sentence))\n",
        "char_dic = {w: i for i, w in enumerate(char_set)} # dictionary 만들기\n",
        "\n",
        "data_dim = len(char_set)\n",
        "hidden_size = len(char_set)\n",
        "num_classes = len(char_set)\n",
        "sequence_length = 10  # Any arbitrary number\n",
        "learning_rate = 0.1\n",
        "\n",
        "dataX = []\n",
        "dataY = []\n",
        "for i in range(0, len(sentence) - sequence_length):\n",
        "    x_str = sentence[i:i + sequence_length]\n",
        "    y_str = sentence[i + 1: i + sequence_length + 1]\n",
        "    print(i, x_str, '->', y_str)\n",
        "\n",
        "    x = [char_dic[c] for c in x_str]  # x str to index\n",
        "    y = [char_dic[c] for c in y_str]  # y str to index\n",
        "\n",
        "    dataX.append(x)\n",
        "    dataY.append(y)\n",
        "\n",
        "batch_size = len(dataX)\n",
        "\n",
        "X = tf.placeholder(tf.int32, [None, sequence_length])\n",
        "Y = tf.placeholder(tf.int32, [None, sequence_length])\n",
        "\n",
        "# One-hot encoding\n",
        "X_one_hot = tf.one_hot(X, num_classes)\n",
        "print(X_one_hot)  # check out the shape\n",
        "\n",
        "\n",
        "# Make a lstm cell with hidden_size (each unit output vector size)\n",
        "def lstm_cell():\n",
        "    cell = rnn.BasicLSTMCell(hidden_size, state_is_tuple=True)\n",
        "    return cell\n",
        "\n",
        "multi_cells = rnn.MultiRNNCell([lstm_cell() for _ in range(2)], state_is_tuple=True)\n",
        "\n",
        "# outputs: unfolding size x hidden size, state = hidden size\n",
        "outputs, _states = tf.nn.dynamic_rnn(multi_cells, X_one_hot, dtype=tf.float32)\n",
        "\n",
        "# FC layer\n",
        "X_for_fc = tf.reshape(outputs, [-1, hidden_size])\n",
        "outputs = tf.contrib.layers.fully_connected(X_for_fc, num_classes, activation_fn=None)\n",
        "\n",
        "# reshape out for sequence_loss\n",
        "outputs = tf.reshape(outputs, [batch_size, sequence_length, num_classes])\n",
        "\n",
        "# All weights are 1 (equal weights)\n",
        "weights = tf.ones([batch_size, sequence_length])\n",
        "\n",
        "sequence_loss = tf.contrib.seq2seq.sequence_loss(\n",
        "    logits=outputs, targets=Y, weights=weights)\n",
        "mean_loss = tf.reduce_mean(sequence_loss)\n",
        "train_op = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(mean_loss)\n",
        "\n",
        "sess = tf.Session()\n",
        "sess.run(tf.global_variables_initializer())\n",
        "\n",
        "for i in range(500):\n",
        "    _, l, results = sess.run(\n",
        "        [train_op, mean_loss, outputs], feed_dict={X: dataX, Y: dataY})\n",
        "    for j, result in enumerate(results):\n",
        "        index = np.argmax(result, axis=1)\n",
        "        print(i, j, ''.join([char_set[t] for t in index]), l)\n",
        "\n",
        "# Let's print the last char of each result to check it works\n",
        "results = sess.run(outputs, feed_dict={X: dataX})\n",
        "for j, result in enumerate(results):\n",
        "    index = np.argmax(result, axis=1)\n",
        "    if j is 0:  # print all for the first result to make a sentence\n",
        "        print(''.join([char_set[t] for t in index]), end='')\n",
        "    else:\n",
        "        print(char_set[index[-1]], end='')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zq0vn-eZZZ7D",
        "colab_type": "text"
      },
      "source": [
        "### 6-2. RNN with Time Series Data\n",
        "*Many to One*\n",
        "\n",
        "Seq = 계산할 날짜"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OFRvETMZepT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tf.set_random_seed(777)  # reproducibility\n",
        "\n",
        "if \"DISPLAY\" not in os.environ:\n",
        "    # remove Travis CI Error\n",
        "    matplotlib.use('Agg')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "def MinMaxScaler(data):\n",
        "    ''' Min Max Normalization\n",
        "    Parameters\n",
        "    ----------\n",
        "    data : numpy.ndarray\n",
        "        input data to be normalized\n",
        "        shape: [Batch size, dimension]\n",
        "    Returns\n",
        "    ----------\n",
        "    data : numpy.ndarry\n",
        "        normalized data\n",
        "        shape: [Batch size, dimension]\n",
        "    References\n",
        "    ----------\n",
        "    .. [1] http://sebastianraschka.com/Articles/2014_about_feature_scaling.html\n",
        "    '''\n",
        "    numerator = data - np.min(data, 0)\n",
        "    denominator = np.max(data, 0) - np.min(data, 0)\n",
        "    # noise term prevents the zero division\n",
        "    return numerator / (denominator + 1e-7)\n",
        "\n",
        "\n",
        "# train Parameters\n",
        "seq_length = 7\n",
        "data_dim = 5\n",
        "hidden_dim = 10\n",
        "output_dim = 1\n",
        "learning_rate = 0.01\n",
        "iterations = 500\n",
        "\n",
        "# Open, High, Low, Volume, Close\n",
        "xy = np.loadtxt('data-02-stock_daily.csv', delimiter=',')\n",
        "xy = xy[::-1]  # reverse order (chronically ordered)\n",
        "\n",
        "# train/test split\n",
        "train_size = int(len(xy) * 0.7)\n",
        "train_set = xy[0:train_size]\n",
        "test_set = xy[train_size - seq_length:]  # Index from [train_size - seq_length] to utilize past sequence\n",
        "\n",
        "# Scale each\n",
        "train_set = MinMaxScaler(train_set)\n",
        "test_set = MinMaxScaler(test_set)\n",
        "\n",
        "# build datasets\n",
        "def build_dataset(time_series, seq_length):\n",
        "    dataX = []\n",
        "    dataY = []\n",
        "    for i in range(0, len(time_series) - seq_length):\n",
        "        _x = time_series[i:i + seq_length, :]\n",
        "        _y = time_series[i + seq_length, [-1]]  # Next close price\n",
        "        print(_x, \"->\", _y)\n",
        "        dataX.append(_x)\n",
        "        dataY.append(_y)\n",
        "    return np.array(dataX), np.array(dataY)\n",
        "\n",
        "trainX, trainY = build_dataset(train_set, seq_length)\n",
        "testX, testY = build_dataset(test_set, seq_length)\n",
        "\n",
        "# input place holders\n",
        "X = tf.placeholder(tf.float32, [None, seq_length, data_dim])\n",
        "Y = tf.placeholder(tf.float32, [None, 1])\n",
        "\n",
        "# build a LSTM network\n",
        "cell = tf.contrib.rnn.BasicLSTMCell(\n",
        "    num_units=hidden_dim, state_is_tuple=True, activation=tf.tanh)\n",
        "outputs, _states = tf.nn.dynamic_rnn(cell, X, dtype=tf.float32)\n",
        "Y_pred = tf.contrib.layers.fully_connected(\n",
        "    outputs[:, -1], output_dim, activation_fn=None)  # We use the last cell's output\n",
        "\n",
        "# cost/loss\n",
        "loss = tf.reduce_sum(tf.square(Y_pred - Y))  # sum of the squares\n",
        "# optimizer\n",
        "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
        "train = optimizer.minimize(loss)\n",
        "\n",
        "# RMSE\n",
        "targets = tf.placeholder(tf.float32, [None, 1])\n",
        "predictions = tf.placeholder(tf.float32, [None, 1])\n",
        "rmse = tf.sqrt(tf.reduce_mean(tf.square(targets - predictions)))\n",
        "\n",
        "with tf.Session() as sess:\n",
        "    init = tf.global_variables_initializer()\n",
        "    sess.run(init)\n",
        "\n",
        "    # Training step\n",
        "    for i in range(iterations):\n",
        "        _, step_loss = sess.run([train, loss], feed_dict={\n",
        "                                X: trainX, Y: trainY})\n",
        "        print(\"[step: {}] loss: {}\".format(i, step_loss))\n",
        "\n",
        "    # Test step\n",
        "    test_predict = sess.run(Y_pred, feed_dict={X: testX})\n",
        "    rmse_val = sess.run(rmse, feed_dict={\n",
        "                    targets: testY, predictions: test_predict})\n",
        "    print(\"RMSE: {}\".format(rmse_val))\n",
        "\n",
        "    # Plot predictions\n",
        "    plt.plot(testY)\n",
        "    plt.plot(test_predict)\n",
        "    plt.xlabel(\"Time Period\")\n",
        "    plt.ylabel(\"Stock Price\")\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}